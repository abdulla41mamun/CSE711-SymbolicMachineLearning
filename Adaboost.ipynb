{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPAp+JgBRLsEL/Gvk/+Rpx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulla41mamun/CSE711-SymbolicMachineLearning/blob/main/Adaboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkH1Bor6Bn_o",
        "outputId": "f3252ef1-049d-4730-b568-0b350d62025a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Dataset with Equal Weights ---\n",
            "     Outlook Temperature Humidity    Wind Play Tennis  sample_weight\n",
            "0      Sunny         Hot     High    Weak          No       0.071429\n",
            "1      Sunny         Hot     High  Strong          No       0.071429\n",
            "2   Overcast         Hot     High    Weak         Yes       0.071429\n",
            "3       Rain        Mild     High    Weak         Yes       0.071429\n",
            "4       Rain        Cool   Normal    Weak         Yes       0.071429\n",
            "5       Rain        Cool   Normal  Strong          No       0.071429\n",
            "6   Overcast        Cool   Normal  Strong         Yes       0.071429\n",
            "7      Sunny        Mild     High    Weak          No       0.071429\n",
            "8      Sunny        Cool   Normal    Weak         Yes       0.071429\n",
            "9       Rain        Mild   Normal    Weak         Yes       0.071429\n",
            "10     Sunny        Mild   Normal  Strong         Yes       0.071429\n",
            "11  Overcast        Mild     High  Strong         Yes       0.071429\n",
            "12  Overcast         Hot   Normal    Weak         Yes       0.071429\n",
            "13      Rain        Mild     High  Strong          No       0.071429\n",
            "\n",
            "========================= ITERATION 1 =========================\n",
            "\n",
            "--- 🔎 Step 1: Calculating GINI Impurity for Each Feature ---\n",
            "GINI(Outlook) = 0.343\n",
            "GINI(Temperature) = 0.440\n",
            "GINI(Humidity) = 0.367\n",
            "GINI(Wind) = 0.429\n",
            "✅ Best feature chosen: 'Outlook' (Lowest GINI)\n",
            "   Stump's classification rules: {'Sunny': -1, 'Overcast': 1, 'Rain': 1}\n",
            "\n",
            "--- 🧮 Step 2: Calculating Error and 'Amount of Say' (alpha) ---\n",
            "The stump misclassified 4 samples.\n",
            "Total Error = Sum of weights of misclassified samples = 4 * (1/14) = 0.2857\n",
            "Amount of Say (alpha_1) = 0.5 * ln((1 - 0.29) / 0.29) = 0.4581\n",
            "\n",
            "--- ⚖️ Step 3: Updating and Normalizing Sample Weights ---\n",
            "New weight for a CORRECTLY classified sample = (1/14) * e^(-0.46) = 0.0452\n",
            "New weight for an INCORRECTLY classified sample = (1/14) * e^(0.46) = 0.1129\n",
            "\n",
            "Sum of all new weights = (10 * 0.0452) + (4 * 0.1129) = 0.9035\n",
            "\n",
            "Normalized weight (Correct) = 0.0452 / 0.9035 = 0.0500\n",
            "Normalized weight (Incorrect) = 0.1129 / 0.9035 = 0.1250\n",
            "\n",
            "--- 🎲 Step 4: Creating New Dataset with Random Sampling ---\n",
            "Cumulative Weight Table for 'Roulette Wheel' Selection:\n",
            "   Play Tennis  normalized_weight  cumulative_weight\n",
            "0           No              0.050              0.050\n",
            "1           No              0.050              0.100\n",
            "2          Yes              0.050              0.150\n",
            "3          Yes              0.050              0.200\n",
            "4          Yes              0.050              0.250\n",
            "5           No              0.125              0.375\n",
            "6          Yes              0.050              0.425\n",
            "7           No              0.050              0.475\n",
            "8          Yes              0.125              0.600\n",
            "9          Yes              0.050              0.650\n",
            "10         Yes              0.125              0.775\n",
            "11         Yes              0.050              0.825\n",
            "12         Yes              0.050              0.875\n",
            "13          No              0.125              1.000\n",
            "\n",
            "Generated 14 random numbers: \n",
            "[0.3745 0.9507 0.732  0.5987 0.156  0.156  0.0581 0.8662 0.6011 0.7081\n",
            " 0.0206 0.9699 0.8324 0.2123]\n",
            "\n",
            "Mapping random numbers to new dataset samples:\n",
            "  Random # 1 (0.3745) -> selects original sample at index 5\n",
            "  Random # 2 (0.9507) -> selects original sample at index 13\n",
            "  Random # 3 (0.7320) -> selects original sample at index 10\n",
            "  Random # 4 (0.5987) -> selects original sample at index 8\n",
            "  Random # 5 (0.1560) -> selects original sample at index 3\n",
            "  Random # 6 (0.1560) -> selects original sample at index 3\n",
            "  Random # 7 (0.0581) -> selects original sample at index 1\n",
            "  Random # 8 (0.8662) -> selects original sample at index 12\n",
            "  Random # 9 (0.6011) -> selects original sample at index 9\n",
            "  Random # 10 (0.7081) -> selects original sample at index 10\n",
            "  Random # 11 (0.0206) -> selects original sample at index 0\n",
            "  Random # 12 (0.9699) -> selects original sample at index 13\n",
            "  Random # 13 (0.8324) -> selects original sample at index 12\n",
            "  Random # 14 (0.2123) -> selects original sample at index 4\n",
            "\n",
            "✅ New Resampled Dataset for Iteration 2:\n",
            "     Outlook Temperature Humidity    Wind Play Tennis\n",
            "0       Rain        Cool   Normal  Strong          No\n",
            "1       Rain        Mild     High  Strong          No\n",
            "2      Sunny        Mild   Normal  Strong         Yes\n",
            "3      Sunny        Cool   Normal    Weak         Yes\n",
            "4       Rain        Mild     High    Weak         Yes\n",
            "5       Rain        Mild     High    Weak         Yes\n",
            "6      Sunny         Hot     High  Strong          No\n",
            "7   Overcast         Hot   Normal    Weak         Yes\n",
            "8       Rain        Mild   Normal    Weak         Yes\n",
            "9      Sunny        Mild   Normal  Strong         Yes\n",
            "10     Sunny         Hot     High    Weak          No\n",
            "11      Rain        Mild     High  Strong          No\n",
            "12  Overcast         Hot   Normal    Weak         Yes\n",
            "13      Rain        Cool   Normal    Weak         Yes\n",
            "\n",
            "This new dataset, which over-samples the 'hard' cases, would now be used to train the second stump.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# A function to calculate the weighted GINI impurity of a single node\n",
        "def calculate_gini(y, sample_weight):\n",
        "    classes = y.unique()\n",
        "    total_weight = sample_weight.sum()\n",
        "    if total_weight == 0:\n",
        "        return 0\n",
        "    impurity = 1.0\n",
        "    for cls in classes:\n",
        "        p_cls = sample_weight[y == cls].sum() / total_weight\n",
        "        impurity -= p_cls**2\n",
        "    return impurity\n",
        "\n",
        "# A function to find the best stump by iterating through features\n",
        "def find_best_stump_and_show_gini(df, y, sample_weight):\n",
        "    \"\"\"Finds the best stump and PRINTS the GINI calculation for each feature.\"\"\"\n",
        "    best_feature = None\n",
        "    min_gini = float('inf')\n",
        "\n",
        "    print(\"\\n--- 🔎 Step 1: Calculating GINI Impurity for Each Feature ---\")\n",
        "    features = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
        "    gini_scores = {}\n",
        "\n",
        "    for feature in features:\n",
        "        weighted_gini = 0.0\n",
        "        for value in df[feature].unique():\n",
        "            subset_indices = df[feature] == value\n",
        "            subset_y = y[subset_indices]\n",
        "            subset_weights = sample_weight[subset_indices]\n",
        "            branch_weight = subset_weights.sum() / sample_weight.sum()\n",
        "            branch_gini = calculate_gini(subset_y, subset_weights)\n",
        "            weighted_gini += branch_weight * branch_gini\n",
        "\n",
        "        gini_scores[feature] = weighted_gini\n",
        "        print(f\"GINI({feature}) = {weighted_gini:.3f}\")\n",
        "\n",
        "    # Find the feature with the minimum GINI score\n",
        "    best_feature = min(gini_scores, key=gini_scores.get)\n",
        "\n",
        "    # Create the stump based on the best feature\n",
        "    stump = {'feature': best_feature, 'predictions': {}}\n",
        "    for value in df[best_feature].unique():\n",
        "        subset_indices = df[best_feature] == value\n",
        "        stump['predictions'][value] = 1 if y[subset_indices].dot(sample_weight[subset_indices]) >= 0 else -1\n",
        "\n",
        "    return stump\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create the FULL, CORRECTED initial dataset\n",
        "data = {\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
        "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
        "    'Play Tennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "y = df['Play Tennis'].apply(lambda x: 1 if x == 'Yes' else -1)\n",
        "N = len(df)\n",
        "\n",
        "# Initialize weights equally\n",
        "df['sample_weight'] = 1 / N\n",
        "\n",
        "print(\"--- Initial Dataset with Equal Weights ---\")\n",
        "print(df)\n",
        "\n",
        "# =================================================================\n",
        "# ITERATION 1\n",
        "# =================================================================\n",
        "print(\"\\n\" + \"=\"*25 + \" ITERATION 1 \" + \"=\"*25)\n",
        "\n",
        "# Step 1: Find the best stump and show GINI calculations\n",
        "stump_1 = find_best_stump_and_show_gini(df, y, df['sample_weight'])\n",
        "print(f\"✅ Best feature chosen: '{stump_1['feature']}' (Lowest GINI)\")\n",
        "print(f\"   Stump's classification rules: {stump_1['predictions']}\")\n",
        "\n",
        "# Step 2: Calculate Error and Alpha\n",
        "print(\"\\n--- 🧮 Step 2: Calculating Error and 'Amount of Say' (alpha) ---\")\n",
        "predictions = df[stump_1['feature']].map(stump_1['predictions'])\n",
        "misclassified_mask = (predictions != y)\n",
        "total_error_1 = df['sample_weight'][misclassified_mask].sum()\n",
        "\n",
        "print(f\"The stump misclassified {misclassified_mask.sum()} samples.\")\n",
        "# FIXED THE LINE BELOW: Replaced the undefined 'num_incorrect' with the correct variable 'misclassified_mask.sum()'\n",
        "print(f\"Total Error = Sum of weights of misclassified samples = {misclassified_mask.sum()} * (1/14) = {total_error_1:.4f}\")\n",
        "\n",
        "epsilon = 1e-10\n",
        "alpha_1 = 0.5 * np.log((1 - total_error_1) / (total_error_1 + epsilon))\n",
        "print(f\"Amount of Say (alpha_1) = 0.5 * ln((1 - {total_error_1:.2f}) / {total_error_1:.2f}) = {alpha_1:.4f}\")\n",
        "\n",
        "# Step 3: Update and Normalize Weights\n",
        "print(\"\\n--- ⚖️ Step 3: Updating and Normalizing Sample Weights ---\")\n",
        "# Calculate new un-normalized weights\n",
        "new_weight_correct = (1/N) * np.exp(-alpha_1)\n",
        "new_weight_incorrect = (1/N) * np.exp(alpha_1)\n",
        "print(f\"New weight for a CORRECTLY classified sample = (1/14) * e^(-{alpha_1:.2f}) = {new_weight_correct:.4f}\")\n",
        "print(f\"New weight for an INCORRECTLY classified sample = (1/14) * e^({alpha_1:.2f}) = {new_weight_incorrect:.4f}\")\n",
        "\n",
        "# Calculate the SUM of all new weights to use for normalization\n",
        "num_correct = N - misclassified_mask.sum()\n",
        "num_incorrect = misclassified_mask.sum()\n",
        "total_new_weight_sum = (num_correct * new_weight_correct) + (num_incorrect * new_weight_incorrect)\n",
        "print(f\"\\nSum of all new weights = ({num_correct} * {new_weight_correct:.4f}) + ({num_incorrect} * {new_weight_incorrect:.4f}) = {total_new_weight_sum:.4f}\")\n",
        "\n",
        "# Calculate normalized weights\n",
        "normalized_weight_correct = new_weight_correct / total_new_weight_sum\n",
        "normalized_weight_incorrect = new_weight_incorrect / total_new_weight_sum\n",
        "print(f\"\\nNormalized weight (Correct) = {new_weight_correct:.4f} / {total_new_weight_sum:.4f} = {normalized_weight_correct:.4f}\")\n",
        "print(f\"Normalized weight (Incorrect) = {new_weight_incorrect:.4f} / {total_new_weight_sum:.4f} = {normalized_weight_incorrect:.4f}\")\n",
        "\n",
        "# Apply these normalized weights to the dataframe\n",
        "df['normalized_weight'] = np.where(misclassified_mask, normalized_weight_incorrect, normalized_weight_correct)\n",
        "\n",
        "# Step 4: Create New Dataset via Resampling\n",
        "print(\"\\n--- 🎲 Step 4: Creating New Dataset with Random Sampling ---\")\n",
        "df['cumulative_weight'] = df['normalized_weight'].cumsum()\n",
        "print(\"Cumulative Weight Table for 'Roulette Wheel' Selection:\")\n",
        "print(df[['Play Tennis', 'normalized_weight', 'cumulative_weight']].round(4))\n",
        "\n",
        "# Generate 14 random numbers\n",
        "random_numbers = np.random.rand(N)\n",
        "print(f\"\\nGenerated 14 random numbers: \\n{np.round(random_numbers, 4)}\")\n",
        "\n",
        "# Find which sample gets selected for each random number\n",
        "new_indices = []\n",
        "for r in random_numbers:\n",
        "    selected_index = df[df['cumulative_weight'] > r].index[0]\n",
        "    new_indices.append(selected_index)\n",
        "\n",
        "print(\"\\nMapping random numbers to new dataset samples:\")\n",
        "for i in range(N):\n",
        "    print(f\"  Random # {i+1} ({random_numbers[i]:.4f}) -> selects original sample at index {new_indices[i]}\")\n",
        "\n",
        "# Create the new dataframe\n",
        "df_new = df.iloc[new_indices].copy()\n",
        "df_new = df_new.drop(columns=['sample_weight', 'normalized_weight', 'cumulative_weight'])\n",
        "df_new = df_new.reset_index(drop=True)\n",
        "\n",
        "print(\"\\n✅ New Resampled Dataset for Iteration 2:\")\n",
        "print(df_new)\n",
        "print(\"\\nThis new dataset, which over-samples the 'hard' cases, would now be used to train the second stump.\")"
      ]
    }
  ]
}